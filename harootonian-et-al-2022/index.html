<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="DSST/NIMH"><title>Harootonian et al. (2022) · OpenCogData</title><meta name="description" content="Successful navigation requires the ability to compute one’s location and heading from incoming multisensory information. Previous work has shown that "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.webp"><link rel="stylesheet" href="/OpenCogData/css/style.css"><link rel="stylesheet" href="/OpenCogData/css/blog_basic.css"><link rel="stylesheet" href="/OpenCogData/css/font-awesome.min.css"><link rel="stylesheet" href="/OpenCogData/css/insight.css"><link rel="stylesheet" href="/OpenCogData/css/search.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/OpenCogData/js/jquery.js"></script><!-- Global site tag (gtag.js) - Google Analytics--><script async src="https://www.googletagmanager.com/gtag/js?id=G-PTJE4Z001J"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-PTJE4Z001J');</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/OpenCogData">Home</a></li><li><a href="/OpenCogData/archives">Archives</a></li><li><a href="/OpenCogData/tags">Tags</a></li><li><a href="/OpenCogData/about">About</a></li><li><a href="/OpenCogData/contribute">Contribute</a></li></div><div class="information"><div class="nav_right_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"></a></li><li><a class="fa fa-search" onclick="openWindow();"></a></li></div><div class="avatar"><img src="/OpenCogData/images/logo.webp" alt="favicon"></div></div></div><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/OpenCogData/images/logo.webp" style="width:175px;" alt="favicon"><h3 title=""><a href="/OpenCogData">OpenCogData</a></h3><div class="description"><p>A collection of publicly available<br>cognitve task datasets</p></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://github.com/nimh-dsst/OpenCogData"><i class="fa fa-github"></i></a></li></ul></div></div><div class="footer"><div class="p"> <span> MIT License </span><i class="fa fa-star"></i><span> DSST/NIMH</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><span>Anatolo </span></div><div class="beian"></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>Harootonian et al. (2022)</a></h3></div><div class="post-subtitle"><h4>Combination and competition between path integration and landmark navigation in the estimation of heading direction</h4></div><div class="post-links"><a target="_blank" rel="noopener" href="https://doi.org/10.1371/journal.pcbi.1009222">[Paper] </a><a target="_blank" rel="noopener" href="https://github.com/sharootonian/CombinationAndCompetitionHeadingDirection"> [Data]</a></div><div class="post-content"><p><p>Successful navigation requires the ability to compute one’s location and heading from incoming multisensory information. Previous work has shown that this multisensory input comes in two forms: body-based idiothetic cues, from one’s own rotations and translations, and visual allothetic cues, from the environment (usually visual landmarks). However, exactly how these two streams of information are integrated is unclear, with some models suggesting the body-based idiothetic and visual allothetic cues are combined, while others suggest they compete. In this paper we investigated the integration of body-based idiothetic and visual allothetic cues in the computation of heading using virtual reality. In our experiment, participants performed a series of body turns of up to 360 degrees in the dark with only a brief flash (300ms) of visual feedback en route. Because the environment was virtual, we had full control over the visual feedback and were able to vary the offset between this feedback and the true heading angle. By measuring the effect of the feedback offset on the angle participants turned, we were able to determine the extent to which they incorporated visual feedback as a function of the offset error. By further modeling this behavior we were able to quantify the computations people used. While there were considerable individual differences in performance on our task, with some participants mostly ignoring the visual feedback and others relying on it almost entirely, our modeling results suggest that almost all participants used the same strategy in which idiothetic and allothetic cues are combined when the mismatch between them is small, but compete when the mismatch is large. These findings suggest that participants update their estimate of heading using a hybrid strategy that mixes the combination and competition of cues.</p>
</p></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-tag"></i><a class="tag" href="/OpenCogData/tags/spatial-navigation/" title="spatial navigation">spatial navigation </a></div></div></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/OpenCogData/haddara-rahnev-2022/" title="Haddara &amp; Rahnev (2022)">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/OpenCogData/wimmer-poldrack-2022/" title="Wimmer &amp; Poldrack (2022)">Next</a></li></ul></div><script src="/OpenCogData/js/visitors.js"></script></div></div></div></div><script src="/OpenCogData/js/jquery-migrate-1.2.1.min.js"></script><script src="/OpenCogData/js/jquery.appear.js"></script><script src="/OpenCogData/js/add-bookmark.js"></script><script>(function(window){var INSIGHT_CONFIG={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)",},CONTENT_URL:"/OpenCogData/content.json",};window.INSIGHT_CONFIG=INSIGHT_CONFIG})(window);</script><script src="/OpenCogData/js/insight.js" defer></script><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="Search..."><span class="searchbox-close"><a class="fa fa-times-circle" onclick="closeWindow();"></a></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"><p>Seraching...</p></div></div></div></div></body></html>