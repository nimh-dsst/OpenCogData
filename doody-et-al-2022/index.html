<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author" content="Niv Lab"><title>Doody et al. (2022) Â· OpenData</title><meta name="description" content="Reinforcement learning (RL) is widely regarded as divisible into two distinct computational strategies. Model-free learning is a simple RL process in "><meta name="keywords"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.webp"><link rel="stylesheet" href="/opendata/css/style.css"><link rel="stylesheet" href="/opendata/css/blog_basic.css"><link rel="stylesheet" href="/opendata/css/font-awesome.min.css"><link rel="stylesheet" href="/opendata/css/insight.css"><link rel="stylesheet" href="/opendata/css/search.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><script src="/opendata/js/jquery.js"></script><!-- Global site tag (gtag.js) - Google Analytics--><script async src="https://www.googletagmanager.com/gtag/js?id=G-PTJE4Z001J"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-PTJE4Z001J');</script><meta name="generator" content="Hexo 6.3.0"></head><body><div class="page-top animated fadeInDown"><div class="nav"><li><a href="/opendata">Home</a></li><li><a href="/opendata/archives">Archives</a></li><li><a href="/opendata/tags">Tags</a></li><li><a href="/opendata/about">About</a></li><li><a href="/opendata/contribute">Contribute</a></li></div><div class="information"><div class="nav_right_btn"><li><a class="fa fa-chevron-left" onclick="window.history.go(-1)"></a></li><li><a class="fa fa-search" onclick="openWindow();"></a></li></div><div class="avatar"><img src="/opendata/images/logo.webp" alt="favicon"></div></div></div><div class="sidebar animated fadeInDown"><div class="sidebar-top"><div class="logo-title"><div class="title"><img src="/opendata/images/logo.webp" style="width:175px;" alt="favicon"><h3 title=""><a href="/opendata">OpenData</a></h3><div class="description"><p>A collection of publicly available<br>behavioral datasets</p></div></div><ul class="social-links"><li><a target="_blank" rel="noopener" href="https://github.com/nivlab/opendata"><i class="fa fa-github"></i></a></li></ul></div></div><div class="footer"><div class="p"> <span> MIT License </span><i class="fa fa-star"></i><span> Niv Lab</span></div><div class="by_farbox"><span>Powered by </span><a href="https://hexo.io/zh-cn/" target="_blank">Hexo </a><span> & </span><span>Anatolo </span></div><div class="beian"></div></div></div><div class="main"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post animated fadeInDown"><div class="post-title"><h3><a>Doody et al. (2022)</a></h3></div><div class="post-subtitle"><h4>Model-based learning retrospectively updates model-free values</h4></div><div class="post-links"><a target="_blank" rel="noopener" href="https://doi.org/10.1038/s41598-022-05567-3">[Paper] </a><a target="_blank" rel="noopener" href="https://osf.io/e4znv/?view_only=0887fd5ba8714b02aad596beca40314b"> [Data]</a></div><div class="post-content"><p><p>Reinforcement learning (RL) is widely regarded as divisible into two distinct computational strategies. Model-free learning is a simple RL process in which a value is associated with actions, whereas model-based learning relies on the formation of internal models of the environment to maximise reward. Recently, theoretical and animal work has suggested that such models might be used to train model-free behaviour, reducing the burden of costly forward planning. Here we devised a way to probe this possibility in human behaviour. We adapted a two-stage decision task and found evidence that model-based processes at the time of learning can alter model-free valuation in healthy individuals. We asked people to rate subjective value of an irrelevant feature that was seen at the time a model-based decision would have been made. These irrelevant feature value ratings were updated by rewards, but in a way that accounted for whether the selected action retrospectively ought to have been taken. This model-based influence on model-free value ratings was best accounted for by a reward prediction error that was calculated relative to the decision path that would most likely have led to the reward. This effect occurred independently of attention and was not present when participants were not explicitly told about the structure of the environment. These findings suggest that current conceptions of model-based and model-free learning require updating in favour of a more integrated approach. Our task provides an empirical handle for further study of the dialogue between these two learning systems in the future.</p>
</p></div><div class="post-footer"><div class="meta"><div class="info"><i class="fa fa-tag"></i></div></div></div></div><div class="pagination"><ul class="clearfix"><li class="pre pagbuttons"><a class="btn" role="navigation" href="/opendata/weber-et-al-2022/" title="Weber et al. (2022)">Previous</a></li><li class="next pagbuttons"><a class="btn" role="navigation" href="/opendata/zang-et-al-2022/" title="Zang et al. (2022)">Next</a></li></ul></div><script src="/opendata/js/visitors.js"></script></div></div></div></div><script src="/opendata/js/jquery-migrate-1.2.1.min.js"></script><script src="/opendata/js/jquery.appear.js"></script><script src="/opendata/js/add-bookmark.js"></script><script>(function(window){var INSIGHT_CONFIG={TRANSLATION:{POSTS:"Posts",PAGES:"Pages",CATEGORIES:"Categories",TAGS:"Tags",UNTITLED:"(Untitled)",},CONTENT_URL:"/opendata/content.json",};window.INSIGHT_CONFIG=INSIGHT_CONFIG})(window);</script><script src="/opendata/js/insight.js" defer></script><div class="searchbox ins-search"><div class="searchbox-container ins-search-container"><div class="searchbox-input-wrapper"><input class="searchbox-input ins-search-input" type="text" placeholder="Search..."><span class="searchbox-close"><a class="fa fa-times-circle" onclick="closeWindow();"></a></span></div><div class="searchbox-result-wrapper ins-section-wrapper"><div class="ins-section-container"><p>Seraching...</p></div></div></div></div></body></html>